# üõí Super Store Data Analytics & ETL Project

![Status](https://img.shields.io/badge/Status-Conclu√≠do-success)
![Python](https://img.shields.io/badge/Python-3.12-blue)
![BigQuery](https://img.shields.io/badge/Google-BigQuery-orange)
![PowerBI](https://img.shields.io/badge/PowerBI-Dashboard-yellow)

## üìã Sobre o Projeto

A **Super Store**, uma l√≠der do setor de varejo, enfrentava o desafio de gerenciar grandes volumes de dados dispersos e desestruturados. O objetivo deste projeto foi implementar um pipeline **ETL (Extract, Transform, Load)** robusto para estruturar esses dados em um **Data Warehouse** no Google BigQuery, permitindo an√°lises estrat√©gicas de padr√µes de consumo e performance de vendas.

O projeto culminou em um Dashboard interativo para suporte √† tomada de decis√£o.

![Dashboard P√°gina 1](dashboard_pag1.png)
![Dashboard P√°gina 2](dashboard_pag2.png)
![Dashboard P√°gina 3](dashboard_pag3.png)

---

## ‚öôÔ∏è Arquitetura e Tecnologias

A solu√ß√£o foi desenvolvida utilizando a arquitetura de **Modelo Estrela (Star Schema)**, ideal para consultas anal√≠ticas r√°pidas.

***Linguagens:** Python (Pandas, Numpy, Requests, BeautifulSoup).
***Cloud & Data Warehouse:** Google BigQuery.
***Visualiza√ß√£o de Dados:** Microsoft Power BI .
***Ambiente de Desenvolvimento:** Google Colab.

---

## üöÄ Etapas do Pipeline ETL

### 1. Extra√ß√£o (Extraction)
A ingest√£o de dados combinou fontes internas e enriquecimento com dados externos:
***Dados Transacionais:** Dataset `superstore.csv` contendo hist√≥rico de vendas, clientes e produtos.
***Web Scraping:** Extra√ß√£o automatizada de dados da Wikip√©dia (*List of supermarket chains*) utilizando `BeautifulSoup` para mapear concorrentes globais.

### 2. Transforma√ß√£o (Transformation)
Nesta etapa, os dados brutos foram higienizados e preparados utilizando Python:
***Limpeza de Dados:** Tratamento de valores nulos e remo√ß√£o de duplicatas.
***Detec√ß√£o de Outliers:** Aplica√ß√£o do m√©todo estat√≠stico **IQR (Interquartile Range)** para identificar anomalias em vari√°veis num√©ricas como `profit` (lucro) e `shipping_cost` (custo de envio).
***Padroniza√ß√£o:** Normaliza√ß√£o de strings (ex: capitaliza√ß√£o de nomes) e convers√£o de tipos de dados (`datetime`).
***Engenharia de Atributos:** Cria√ß√£o da dimens√£o `Dim_Tempo` com deriva√ß√£o de Ano, Trimestre, M√™s e Dia da Semana.

### 3. Modelagem de Dados (Star Schema)
Os dados foram estruturados em tabelas Fato e Dimens√£o:

| Tabela | Tipo | Descri√ß√£o |
| :--- | :--- | :--- |
| **Fato_Vendas** | Fato |Tabela central contendo m√©tricas (Vendas, Lucro, Quantidade) e chaves estrangeiras. |
| **Dim_Cliente** | Dimens√£o | Dados cadastrais. Implementa√ß√£o de **SCD Tipo 2 (Slowly Changing Dimension)** para rastrear hist√≥rico de mudan√ßas no segmento do cliente. |
| **Dim_Produto** | Dimens√£o | Detalhes de produtos, categorias e subcategorias. |
| **Dim_Localidade** | Dimens√£o | Hierarquia geogr√°fica completa (Cidade, Estado, Pa√≠s, Regi√£o, Mercado). |
| **Dim_Tempo** | Dimens√£o | Calend√°rio mestre para an√°lises temporais. |

### 4. Carga (Load)
Carregamento automatizado das tabelas transformadas para o dataset `project_rota01` no **Google BigQuery**, respeitando a integridade referencial e a ordem de depend√™ncia das tabelas (Dimens√µes primeiro, Fato depois).

---

## üìä Estrutura do Dashboard

O painel no Power BI foi estruturado em tr√™s n√≠veis de an√°lise.

1.**Vis√£o Executiva (KPIs):** Monitoramento macro de Vendas Totais, Lucro L√≠quido, Margem de Lucro e Custos Log√≠sticos.
2.**An√°lise Temporal:** Evolu√ß√£o de vendas e lucro (YoY/MoM) e identifica√ß√£o de sazonalidade mensal.
3.**Vis√£o Detalhada:** Mapa de vendas por regi√£o e performance por Categoria de Produto e Segmento de Cliente.

---

## üõ†Ô∏è Como Executar

1.  **Pr√©-requisitos:**
    * Conta no Google Cloud Platform (GCP).
    * Python 3.x instalado.
    * Bibliotecas: `pandas`, `google-cloud-bigquery`, `requests`, `beautifulsoup4`.

2.  **Configura√ß√£o:**
    * Clone o reposit√≥rio.
    * Configure as credenciais do GCP (`GOOGLE_APPLICATION_CREDENTIALS`).
    * [cite_start]Atualize as vari√°veis `PROJECT_ID` e `DATASET_ID` no script Python.

3.  **Execu√ß√£o:**
    * Execute o notebook `Rota01laboratoria.ipynb` para rodar o pipeline ETL completo.
    * Abra o arquivo `.pbix` no Power BI e atualize a fonte de dados para o seu projeto no BigQuery.

---

## ‚úíÔ∏è Autoria

Desenvolvido como projeto de aprofundamento em Engenharia de Dados e BI.